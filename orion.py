# -*- coding: utf-8 -*-
"""orion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gKsu39G1oQq5wfHIQAUT1FWTI2CxAuH0
"""

import tensorflow as tf
# მივიღოთ GPU-ს სახელი
device_name = tf.test.gpu_device_name()
# დავაფორმატოთ output-ი
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')

import torch
# თუ GPU ნაპოვნია
if torch.cuda.is_available():    
    # ვუბრძანოთ pytorch-ს რომ გამოიყენოს იგი    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

!pip install transformers

import pandas as pd
# ჩავტვირთოთ მონაცემები pandas-ის dataframe-ში.
df = pd.read_csv("./data.txt", delimiter=',', header=None, names=['sentence', 'label'])
# დავბეჭდოთ დასასწავლი წინადადებების რაოდენობა
print('Number of training sentences: {:,}\n'.format(df.shape[0]))
# ვნახოთ რამდენიმე დასასწავლი წინადადების მაგალითი
df.sample(2)

# ცალ-ცალკე მასივებში ჩავსვათ წინადადებები და მათი კლასები
sentences = df.sentence.values
labels = df.label.values
print(sentences)
print(labels)

from transformers import BertTokenizer
# ჩამოვტვირთოთ BERT-ის ტოკენიზატორი.
print('Loading BERT tokenizer...')
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# მოვახდინოთ ყველა წინადადების ტოკენიზაცია და ავსახოთ მათი ტოკენი სიტყვის ID-ში
input_ids = []

for sent in sentences:
    # `encode` ფუნქცია იზამს შემდეგს:
    #   (1) მოახდენს წინადადების ტოკენიზაციას
    #   (2) დასაწყისში  `[CLS]` ტოკენს ჩასვამს
    #   (3) ბოლოში მიადგამს `[SEP]` ტოკენს.
    #   (4) ასახავს ტოკენს თავის ID-ში
    encoded_sent = tokenizer.encode(
                        sent,                      # დასამუშავებელი წინადადება
                        add_special_tokens = True, # ჩაამატე '[CLS]' და '[SEP]'
                   )
    
    # დამუშავებული წინადადებები ჩავამატოთ სიაში
    input_ids.append(encoded_sent)

# დავბეჭდოთ მენულე წინადადება ახლა უკვე როგორც ID-ების სია.
print('Original: ', sentences[0])
print('Token IDs:', input_ids[0])

sent = "ბალანსი მაჩვენე ჯიგარო"

# ინგლისური სიტყვების ტოკენიზაცია მუშაობს
print(tokenizer.convert_tokens_to_ids(["car"]))

# დავბეჭდოთ წინადადება sent-ის ტოკენიზაციის შედეგი
print(tokenizer.tokenize(sent))

# ასო 'ბ'-ს ID არის 1565
print(tokenizer.convert_tokens_to_ids(["ბ"]))

# ID-ების სიების მაქსიმალური სიგრძე
print('Max sentence length: ', max([len(sen) for sen in input_ids]))

# ვიხმაროთ keras-ის დამხმარე ფუნქცია `pad_sequences` რომ ყველა სიას იგივე ზომა მივცეთ
from keras.preprocessing.sequence import pad_sequences

# დავაწესოთ წინადადების მაქსიმალური სიგრძე (სიტყვაზე 120)
MAX_LEN = 120

print('\nPadding/truncating all sentences to %d values...' % MAX_LEN)
print('\nPadding token: "{:}", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))
# [PAD] ტოკენის ID არის 0, ამიტომ შევავსოთ ყველა სია 120 სიგრძემდე 0-ებით
# "post" ანიშნებს, რომ [PAD] ტოკენები გვინდა ჩავსვათ სიის ბოლოში და არა თავში
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", 
                          value=0, truncating="post", padding="post")
print('\Done.\n\n')

print("Example: \n\n\n",input_ids[0])

# შევქმნათ სია, რომელშიც გვექნება ყველა წინადადების ყურადღების mask-ები
attention_masks = []

for sent in input_ids:
    
    # შევქმნათ ყურადღების mask-ი:
    #   თუ ტოკენის ID არის 0, მაშინ [PAD] ტოკენი ყოფილა და mask არის 0
    #   თუ ტოკენის ID არის  > 0, მაშინ არაა [PAD] ტოკენი და mask არის 1
    att_mask = [int(token_id > 0) for token_id in sent]
    
    attention_masks.append(att_mask)

# ვიხმაროთ train_test_split რომ დავყოთ წინადადებები დასასწავლ და გასატესტ ნაწილებად
from sklearn.model_selection import train_test_split
# ვიხმაროთ 90% სწავლებისთვის და 10% ტესტირებისთვის
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, 
                                                            random_state=2021, test_size=0.1)
# იგივე ვქნათ mask-ებისთვის
train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,
                                             random_state=2021, test_size=0.1)

# ყველაფერი ვაქციოთ pytorch-ის ტენზორებად (სწავლებისთვის საჭიროა)
train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)
validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
# DataLoader-ს წინასწარ უნდა ვუთხრათ batch-ების ზომა რომელსაც სწავლებისას ვიხმართ
# რადგან ჩვენ გადასწავლებას ვახდენთ, პატარა რიცხვიც კმარა (როგორიცაა 32)
batch_size = 32

# შევქმნათ DataLoader-ი დასასწავლი წინადადებებისთვის
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# შევქმნათ DataLoader-ი გასატესტი წინადადებებისთვის
validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

from transformers import BertForSequenceClassification, AdamW, BertConfig
# ჩამოვტვირთოთ BertForSequenceClassification, წინასწარ ნასწავლი BERT-ის მოდელი
# რომელსაც თავზე აქვს მიდგმული კლასიფიქატორი ქსელი

model = BertForSequenceClassification.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels = 2, # კლასების რაოდენობა - ჯერჯერობით 2 კლასი გვაქვს   
    output_attentions = False, # არ დავაბრუნოთ attention weight-ები
    output_hidden_states = False, # არ დავაბრუნოთ ფარული მეხსიერება
)
# ვიხმაროთ GPU გამოთვლებისთვის
model.cuda()

# დავბეჭდოთ ბერტის პარამეტრები
params = list(model.named_parameters())
print('The BERT model has {:} different named parameters.\n'.format(len(params)))
print('==== Embedding Layer ====\n')
for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))
print('\n==== First Transformer ====\n')
for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))
print('\n==== Output Layer ====\n')
for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

optimizer = AdamW(model.parameters(),
                  lr = 2e-5, # args.learning_rate
                  eps = 1e-8 # args.adam_epsilon
                )
from transformers import get_linear_schedule_with_warmup

# ვთ. გვაქვს 4 ეპოქა. თითო ეპოქაში დასასწავლ მონაცემებიდან ყველას
# ერთხელ მაინც ექნება საშუალება რომ გავლენა იქონიოს ქსელის პარამეტრების ცვლილებაზე
epochs = 4

# total_steps = batches * epochs, სადაც batch არის დასასწავლ მონაცემთა რაღაც ქვესიმრავლე
# რომელზეც ეპოქის დროს ისწავლის ქსელი
total_steps = len(train_dataloader) * epochs

# scheduler განსაზღვრავს სწავლების თანმიმდევრობას
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)